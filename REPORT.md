# Midterm Project Report
**Author**: Junhui Cho

**Due Date**: March 30, 2025

## AI Disclosure

### General Usage

Primary Generative AI Tool: ChatGPT 4o

Generative AI was mainly used for research purposes. Following is the detailed list of how I utilized the tool:

1. List of CNN-based models
2. List of Optimizers
3. List of Learning Rate Schedulers
4. Suggetions for generalization
    - For the context, I have provided my training script as an input, and asked for suggestions to mitigate overfitting and improve generalization.

### AI-Generated Code

**Design of Head layer for transfer learning**:

In the beginning, I started simply with a fully connected layer, but I wanted to test if some change in head layer could help to mitigate overfitting. I requested for dropout, and additional pooling layer.

```bash
class HeadWithPooling(nn.Module):
    def __init__(self, in_features, num_classes):
        super().__init__()
        self.bn1 = nn.BatchNorm1d(in_features)
        self.dropout1 = nn.Dropout(0.5)
        self.linear1 = nn.Linear(in_features, 512)
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.flatten = nn.Flatten()
        self.linear2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.bn1(x)
        x = self.dropout1(x)
        x = self.linear1(x)
        x = self.relu(x)
        x = x.unsqueeze(-1)      # (B, 512) -> (B, 512, 1)
        x = self.pool(x)         # (B, 512, 1)
        x = self.flatten(x)      # (B, 512)
        x = self.linear2(x)      # (B, 100)
        return x
```

**Functions for freezing and unfreezing backbone**:

To test if freezing the backbone for first few epochs helps, I requested for helper functions that freeze and unfreeze parameter updates.

```bash
def freeze_backbone(model):
    for name, param in model.named_parameters():
        if "fc" not in name:
            param.requires_grad = False

def unfreeze_all(model):
    for param in model.parameters():
        param.requires_grad = True
```

Remaining parts of the code are generated by myself.

## Part 1: Simple CNN

### Model Description


Following is the design for simple CNN model. I tried to minimalize the model so that I can observe the baseline performance of the simplest form of CNN-based architecture.

```bash
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # TODO - define the layers of the network you will use
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)

        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.fc1 = nn.Linear(32 * 8 * 8, 256)
        self.fc2 = nn.Linear(256, 100)
        
    def forward(self, x):
        # TODO - define the forward pass of the network you will use
        x = self.pool(F.relu(self.conv1(x))) # Pooling + Convolution 1
        x = self.pool(F.relu(self.conv2(x))) # Pooling + Convolution 2
        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor
        x = F.relu(self.fc1(x)) # Activation
        x = self.fc2(x) # Output layer

        return x
```

#### Components
- Convolution Layers:
    - `conv1`, `conv2`
- Pooling Layer:
    - `pool`
- Fuclly Connected Linear Layers:
    - `fc1`, `fc2`

### Configuration, Augmentation, Loss Function, Optimizer, and Learning Scheduler

For the base model, I ran it on my local machine, so I trained it just for 5 epochs. For the loss function, I used `CrossEntropyLoss()`, which is a commonly used loss function for multi-class classification problems. No additonal arguments were included to test the baseline performance. `Adam` was chosen for the optimizer as it is one of thje most popular optimization algorithms used to train deep learning models.

Hyperparameter tuning and regularization were not involved in part 1.

Data Augmenation was also not applied, and only the given normalization was applied.


```bash
CONFIG = {
        "model": "MyModel",   # Change name when using a different model
        "batch_size": 8, # run batch size finder to find optimal batch size
        "learning_rate": 0.001,
        "epochs": 5,  # Train for longer in a real scenario
        "num_workers": 4, # Adjust based on your system
        "device": "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu",
        "data_dir": "./data",  # Make sure this directory exists
        "ood_dir": "./data/ood-test",
        "wandb_project": "sp25-ds542-challenge",
        "seed": 42,
    }

transform_train = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Example normalization
    ])

transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Example normalization
    ])   ### TODO -- BEGIN SOLUTION

criterion = nn.CrossEntropyLoss()   ### TODO -- define loss criterion
optimizer = optim.Adam(model.parameters(), CONFIG['learning_rate'])
scheduler = None
```

### Results

#### Learning Curves

![Alt text](images/part1.png)

#### Leader


### Part 2

### Part 3
